{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOihMwk6qHLKTQvzUHCHkGn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9dvOqcabRUKJ"},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')\n","\n","import nltk.data\n","from nltk.corpus import wordnet as wn\n","from nltk.tokenize import word_tokenize\n","import matplotlib.pyplot as plt\n","\n","\n","def get_sentences_from_corpus(_filename):\n","    sents = []\n","    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n","    for line in open(_filename, 'r').readlines():\n","        sent_list = sent_detector.tokenize(line.strip())\n","        for sent in sent_list: sents.append(sent)\n","    return sents\n","\n","\n","def compute_distinct_combo_per_sent(_sent, _print_senses=False):\n","    # tokenize a sentence to analyse each word's available senses\n","    tokens  = word_tokenize(_sent)\n","    distinct_combo = 1\n","    for token in tokens:\n","        senses = wn.synsets(token)\n","        if _print_senses:\n","            print(f'\\n{token}:')\n","            for sense in senses:\n","                print(f'{sense}: {sense.definition()}')\n","            continue\n","        n_senses = len(senses)\n","        # if a word returns 0 sense, convert 0 to 1 since there is\n","        # at least one sense for this word according to this corpus\n","        n_senses = n_senses if n_senses != 0 else 1\n","        distinct_combo *= n_senses\n","    return len(tokens), distinct_combo\n","\n","\n","def simplified_lesk(_word, _sent, _gloss):\n","    best_sense = None\n","    overlaps = []\n","    max_overlap = -1\n","    context = _sent\n","    senses = wn.synsets(_word)\n","    signature = _gloss + \" \"\n","    for sense in senses:\n","\n","        signature += sense.definition()\n","\n","        if len(sense.examples()) > 0:\n","\n","            signature += ' '\n","            examples = sense.examples()\n","            for i in range(len(examples)):\n","                signature += examples[i]\n","\n","        # compute number of overlap\n","        overlap = 0\n","        tokenized_signature = word_tokenize(signature)\n","\n","        for i in range(len(context)):\n","            for j in range(len(tokenized_signature)):\n","                if context[i] == tokenized_signature[j]:\n","                    overlap += 1\n","\n","        # check if overlap is more than max_overlap\n","        if overlap > max_overlap:\n","            max_overlap = overlap\n","            best_sense = sense\n","\n","        overlaps.append(overlap)\n","\n","    return best_sense, overlaps, senses\n","\n","\n","# Response to 23.1\n","# Using WordNet or any standard dictionary, determine how many\n","# senses there are for each of the open-class words in each sentence.\n","# How many distinct combinations of senses are there for each sentence?\n","# How does this number seem to vary with sentence length?\n","show_res = False\n","if show_res:\n","    length_of_sent = []\n","    dist_combos = []\n","    sentences = get_sentences_from_corpus('small-corpus.txt')\n","    for sentence in sentences:\n","        sent_len, dist_combo = compute_distinct_combo_per_sent(sentence)\n","        if sent_len > 55: continue\n","        length_of_sent.append(sent_len)\n","        dist_combos.append(dist_combo)\n","    plt.plot(length_of_sent, dist_combos, 'rx')\n","    plt.xlabel('Sentence Length')\n","    plt.ylabel('Unique Combination of Senses')\n","    plt.show()\n","\n","\n","# Response to 23.2\n","show_senses = False\n","if show_senses:\n","    sentences = get_sentences_from_corpus('small-corpus.txt')\n","    for sentence in sentences:\n","        sent_len, dist_combo = compute_distinct_combo_per_sent(sentence, True)\n","\n","\n","# Response to 23.3\n","run_lesk = True\n","if run_lesk:\n","\n","    print('\\nSimple Lesk')\n","\n","    test_sent = \"Time flies like an arrow\"\n","    tokenized_sent = word_tokenize(test_sent)\n","\n","    gloss = \"\"\n","    for word in tokenized_sent:\n","        best_sense, _, _ = simplified_lesk(word, test_sent, gloss)\n","        if best_sense is not None:\n","            print(f'{word}: {best_sense.definition()}')\n","            gloss += \" \" + best_sense.definition()\n","        else:\n","            print(f'{word}: word sense not found')\n","\n","# not a response to any textbook exercise...\n","# just an experiment to see if collecting\n","# overlap bidirectionally helps with accuracy\n","\n","# this bidirectional method seems to have\n","# the same performance as the simple version.\n","\n","run_bidirectional_lesk = True\n","if run_bidirectional_lesk:\n","\n","    print('\\nBidirectional Lesk')\n","\n","    test_sent = \"Time flies like an arrow\"\n","    tokenized_sent = word_tokenize(test_sent)\n","    overlap_count_dict = {}\n","\n","    gloss = \"\"\n","    for word in tokenized_sent:\n","        best_sense, overlaps, senses = simplified_lesk(word, test_sent, gloss)\n","        overlap_count_dict.update({word: {}})\n","        for i in range(len(senses)):\n","            overlap_count_dict[word].update({senses[i]: overlaps[i]})\n","\n","    gloss = \"\"\n","    for word in reversed(tokenized_sent):\n","        best_sense, overlaps, senses = simplified_lesk(word, test_sent, gloss)\n","        for i in range(len(senses)):\n","            overlap_count_dict[word][senses[i]] += overlaps[i]\n","\n","    print(overlap_count_dict)\n","    for word in tokenized_sent:\n","        overlap_count_dict[word] = dict(\n","            sorted(overlap_count_dict[word].items(),\n","                   key=lambda item: item[1], reverse=True ))\n","        if len(list(overlap_count_dict[word].keys())) > 0:\n","            best_sense = \\\n","                list(overlap_count_dict[word].keys())[0].definition()\n","        else:\n","            best_sense = 'NOT FOUND'\n","        print(f'{word}: {best_sense}')\n","\n","# run simple lesk on the entire small_corpus.txt\n","run_lesk_on_small_corpus = False\n","if run_lesk_on_small_corpus:\n","    run_for_n_sent = 5\n","    sentences = get_sentences_from_corpus('small-corpus.txt')\n","    i = 0\n","    gloss = \"\"\n","    for sentence in sentences:\n","        print(f'\\n{sentence}\\n')\n","        tokenized_sent = word_tokenize(sentence)\n","        for word in tokenized_sent:\n","            best_sense, _, _ = simplified_lesk(word, sentence, gloss)\n","            if best_sense is not None:\n","                print(f'{word}: {best_sense.definition()}')\n","                gloss += \" \" + best_sense.definition()\n","            else: print(f'{word}: word sense not found')\n","        i += 1\n","        if i > run_for_n_sent: break"]},{"cell_type":"code","source":[],"metadata":{"id":"DOgS_bhJRV6a"},"execution_count":null,"outputs":[]}]}