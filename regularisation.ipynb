{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "oFG5rX7yTK7q",
        "outputId": "6491c9a0-e7bd-4dbc-b112-ff092a6da407"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './lexicon/positive.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1d7f93c339fe>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m                 {'review': 'Good movie but a little too long. I liked the first one better but the special effects were better in part 2.', 'class': 'neu'}]\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mpos_lexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./lexicon/positive.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'pos_lexicon: {pos_lexicon}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mneg_lexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./lexicon/negative.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-1d7f93c339fe>\u001b[0m in \u001b[0;36mtokenize_file\u001b[0;34m(_filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdearray_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lexicon/positive.txt'"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def tokenize_file(_filename):\n",
        "    tokens = []\n",
        "    dearray_tokens = []\n",
        "    for line in open(_filename, 'r').readlines():\n",
        "        words = [word.lower() for word in line.split() if len(word) >= 2]\n",
        "        tokens.append(words)\n",
        "    for word in tokens:\n",
        "        dearray_tokens.append(word[0])\n",
        "    return dearray_tokens\n",
        "\n",
        "def generate_features(_sample, _pos_lexicon, _neg_lexicon):\n",
        "    pos_cnt = 0\n",
        "    neg_cnt = 0\n",
        "    has_no = False\n",
        "    has_switch = False\n",
        "    pron_cnt = 0\n",
        "    has_exc = False\n",
        "\n",
        "    for token in _sample:\n",
        "\n",
        "        token = token.lower()\n",
        "\n",
        "        for word in _pos_lexicon:\n",
        "            if word == token: pos_cnt += 1\n",
        "\n",
        "        for word in _neg_lexicon:\n",
        "            if word == token: neg_cnt += 1\n",
        "\n",
        "        if token == 'no': has_no = True\n",
        "        if token == 'but' or token == 'though' or token == 'although' or token == 'however': has_switch = True\n",
        "        if token == '!': has_exc = True\n",
        "        if token == 'i' or token == 'me' or token == 'you': pron_cnt += 1\n",
        "\n",
        "    return [pos_cnt, neg_cnt, has_no, has_switch, pron_cnt, has_exc, math.log(len(_sample))]\n",
        "\n",
        "def scaling_features(_X):\n",
        "    # 0, 1, 3 are counts; 2, 4 are boolean\n",
        "    # 5 is ln(len(sample)) -> scale as counts\n",
        "    X = np.array(_X)\n",
        "    means = np.mean(X, axis=0)\n",
        "    stds = np.std(X, axis=0)\n",
        "    print(f'\\n\\n\\nX.shape: {X.shape}\\nmeans.shape: {means.shape}\\nstds.shape: {stds.shape}\\n')\n",
        "    print(f'means: {means}\\nstds: {stds}\\n\\n\\n')\n",
        "\n",
        "    scale_index = [0, 1, 4, 6]\n",
        "    for index in scale_index:\n",
        "        for row in X:\n",
        "            row[index] = (row[index] - means[index]) / (stds[index]+0.0001)\n",
        "\n",
        "    return X\n",
        "\n",
        "def softmax(_x, _W, _CLASSES):\n",
        "    nominators = []\n",
        "    denominator = 0\n",
        "    for i in range(len(_CLASSES)):\n",
        "        nominators.append(np.exp(np.dot(_W[i][:-1], _x) + _W[i][-1]))\n",
        "        denominator += nominators[i]\n",
        "    probabilities = np.array(nominators) / denominator\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "def get_true_state(_true_class, _CLASSES):\n",
        "    true_index = np.argwhere(_CLASSES == _true_class)\n",
        "    y = np.zeros(len(_CLASSES))\n",
        "    y[true_index] = 1\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def get_theta_via_cross_entropy(_W, _Xp, _probabilities, _y, _theta, _LEARNING_RATE, _CLASS_SIZE):\n",
        "    loss = -((_y * np.log(_probabilities)) + ((1 - _y) * np.log(1 - _probabilities)))\n",
        "    difference = _probabilities - _y\n",
        "    new_theta = []\n",
        "\n",
        "    for i in range(_CLASS_SIZE):\n",
        "\n",
        "        gradient = np.array([])\n",
        "        for j in range(len(_Xp)):\n",
        "            gradient = np.append(gradient, difference[i] * _Xp[j])\n",
        "\n",
        "        gradient = np.append(gradient, difference[i])\n",
        "        new_theta.append(_theta[i] - (_LEARNING_RATE * gradient))\n",
        "\n",
        "    return loss, np.array(new_theta)\n",
        "\n",
        "\n",
        "def regularization(_theta):\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    training_set = [{'review': 'The film is GORGEOUS but I sure would have appreciated more about the planet and local wildlife and less warfare.', 'class': 'pos'},\n",
        "                    {'review': 'Great story Line. James Cameron did it again!', 'class': 'pos'},\n",
        "                    {'review': 'Movie was beautifully done and had a strong story', 'class': 'pos'},\n",
        "                    {'review': 'Too long, needed less weapon fighting, I think. Rest of show was beautiful and well done! Loved the baby and children scenes!', 'class': 'pos'},\n",
        "                    {'review': 'Astonishing! Enthralling! Exciting! Immersive! None of these words could sensibly be applied to the three-and-a-quarter-hour Wet Smurfahontas stodgeathon that is Avatar: The Way of Water.', 'class': 'neg'},\n",
        "                    {'review': 'I was in awe of the cinematography and animation but felt the story line was awful especially when the harpooned all the whales!! I had to walk out-very disappointed!', 'class': 'neg'},\n",
        "                    {'review': 'none of its characters feel whole, even after three full hours', 'class': 'neg'},\n",
        "                    {'review': 'Graphics of course were amazing but the story was blah, no surprises and it just dragged on WAY too long.', 'class': 'neg'},\n",
        "                    {'review': 'The movie could have been alot shorter and still gotten the story in.', 'class': 'neu'},\n",
        "                    {'review': 'visually stunning but the film lacked soul.', 'class': 'neu'},\n",
        "                    {'review': \"Avatar is still Avatar. Nothing new and nothing worse to call it bad, is somewhere in the middle. The new movie is an expansion of the Pandora universe and shows how would these people live in water. Its visual effect is just as good as the first one. This means that more than 10 years later James Cameron has successfully delivered the same effect to the screen and didn't offer us anything new, while we watch similar technology become commonplace amongst other movies and game titles. This movie would be much appreciated if it came out much sooner after the first one. This is not to say that it is a bad movie, but I don't think it was worthy to watch when it should have been delivered a long time ago, and especially when it takes around 3 hours to finish.\", 'class': 'neu'}]\n",
        "\n",
        "    test_set = [{'review': \"The movie Avatar was amazing and spiritual. It's was a perfect 10\", 'class': 'pos'},\n",
        "                {'review': 'It was an all around amazing experience:) The story and visuals were great. If you liked the original then this one is a must see!', 'class': 'pos'},\n",
        "                {'review': \"I was bored. It wasn't realistic. How is a human military going to lose to bows and arrows?\", 'class': 'neg'},\n",
        "                {'review': 'Too much violence and they killed a whale during the movie. It was all unnecessary.', 'class': 'neg'},\n",
        "                {'review': 'Good movie but a little too long. I liked the first one better but the special effects were better in part 2.', 'class': 'neu'}]\n",
        "\n",
        "    pos_lexicon = tokenize_file('./lexicon/positive.txt')\n",
        "    print(f'pos_lexicon: {pos_lexicon}')\n",
        "    neg_lexicon = tokenize_file('./lexicon/negative.txt')\n",
        "    print(f'neg_lexicon: {neg_lexicon}')\n",
        "\n",
        "    # return [pos_cnt, neg_cnt, has_no, pron_cnt, has_exc, math.log(len(_sample))]\n",
        "    FEATURE_DESCRIPTION = ['pos count', 'neg count', 'has no', 'has (but|although|though)', '1st & 2nd person pronounce count', 'has !', 'length of sample']\n",
        "    CLASSES = np.array(['pos', 'neg', 'neu'])\n",
        "    CLASS_SIZE = len(CLASSES)\n",
        "    FEATURE_SIZE = len(FEATURE_DESCRIPTION)\n",
        "    LEARNING_RATE = 1.0\n",
        "\n",
        "    # create weight and bias vector with He initialization\n",
        "    # each class has its own weights for each feature, and a scalar bias\n",
        "    # W = np.array([[random.uniform(0, 1) * math.sqrt(2)] * (FEATURE_SIZE + 1) for i in range(CLASS_SIZE)])\n",
        "    W = np.array([[0.2] * (FEATURE_SIZE + 1) for i in range(CLASS_SIZE)])\n",
        "    theta = np.array([[0.0] * (FEATURE_SIZE + 1) for i in range(CLASS_SIZE)])\n",
        "    X = []\n",
        "\n",
        "    print(\"\\nTRAINING...\\n\")\n",
        "    for sample in training_set:\n",
        "        sample_tokens = word_tokenize(sample['review'])\n",
        "        print(f\"Tokens:             {sample_tokens}\")\n",
        "        x = generate_features(sample_tokens, pos_lexicon, neg_lexicon)\n",
        "        print(f\"Features:           {x}\")\n",
        "        X.append(x)\n",
        "\n",
        "    Xp = scaling_features(X)\n",
        "    print(f\"Features scaled:    {Xp}\")\n",
        "    current_sample = 0\n",
        "    for sample in training_set:\n",
        "        xp = Xp[current_sample]\n",
        "        P = softmax(xp, W, CLASSES)\n",
        "        print(f\"after softmax:      {P}\")\n",
        "        y = get_true_state(sample['class'], CLASSES)\n",
        "        print(f'true state:         {y}')\n",
        "        loss, theta = get_theta_via_cross_entropy(W, xp, P, y, theta, LEARNING_RATE, CLASS_SIZE)\n",
        "        print(f'loss:               {loss}')\n",
        "        print(f'theta:              {theta}')\n",
        "        current_sample += 1\n",
        "\n",
        "    W = W + theta\n",
        "\n",
        "    print(\"\\nTESTING...\\n\")\n",
        "\n",
        "    X = []\n",
        "    accuracy = 0\n",
        "    for sample in test_set:\n",
        "        sample_tokens = word_tokenize(sample['review'])\n",
        "        print(f\"Tokens:             {sample_tokens}\")\n",
        "        x = generate_features(sample_tokens, pos_lexicon, neg_lexicon)\n",
        "        print(f\"Features:           {X}\")\n",
        "        X.append(x)\n",
        "\n",
        "    Xp = scaling_features(X)\n",
        "    print(f\"Features scaled:    {Xp}\")\n",
        "\n",
        "    current_sample = 0\n",
        "    for sample in test_set:\n",
        "\n",
        "        xp = Xp[current_sample]\n",
        "        P = softmax(xp, W, CLASSES)\n",
        "        print(f\"after softmax:      {P}\")\n",
        "        y_hat = np.argmax(P)\n",
        "        y = get_true_state(sample['class'], CLASSES)\n",
        "\n",
        "        if y[y_hat] == 1:\n",
        "            print(f\"correct:        class {sample['class']} classified as {CLASSES[y_hat]}\")\n",
        "            accuracy += 1\n",
        "        else:\n",
        "            print(f\"wrong:          class {sample['class']} classified as {CLASSES[y_hat]}\")\n",
        "\n",
        "        current_sample += 1\n",
        "\n",
        "    accuracy /= len(test_set)\n",
        "    print(f'accuracy:               {accuracy}')"
      ]
    }
  ]
}