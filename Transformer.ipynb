{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHjQX9QSGCLBSTQk0BaJP7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68jfzUEvN8ny","executionInfo":{"status":"ok","timestamp":1713075106119,"user_tz":-330,"elapsed":7486,"user":{"displayName":"Aishwarya S Yeli","userId":"06362993246759157271"}},"outputId":"771f6f96-fbfc-49eb-edf4-1fe6d53947e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package comtrans to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Resumption\n","of\n","the\n","session\n","I\n","declare\n","<AlignedSent: 'I declare resumed th...' -> 'Je dÃ©clare reprise l...'>\n","0-0 1-1 2-2 3-3 4-4 5-5 6-5 7-7 8-6 9-8 9-9 9-10 9-11 10-12 11-13 12-14 13-15 14-16 16-17 17-18 18-19 19-20 20-20 21-20 29-24 31-25 32-26 33-27 33-28 34-29 35-30 36-20 36-31 37-20 37-21 37-22 37-23 37-32 38-32 39-33\n"]}],"source":["import math\n","from transformers import BertTokenizer, BertModel\n","import pandas as pd\n","import numpy as np\n","import torch\n","from scipy.spatial.distance import cosine\n","\n","import nltk\n","nltk.download('comtrans')\n","from nltk.corpus import comtrans\n","words = comtrans.words('alignment-en-fr.txt')\n","als = comtrans.aligned_sents('alignment-en-fr.txt')\n","for word in words[:6]:\n","    print(word)\n","print(als[1])\n","print(als[1].alignment)\n","\n","\n","def compute_softmax(_arr):\n","    softmax = np.array(len(_arr))\n","    sum = 0.0\n","    for i in range(np.shape(_arr)[0]):\n","        for j in range(np.shape(_arr)[1]):\n","            val = math.exp(_arr[i][j])\n","            sum += math.exp(val)\n","            softmax[i] = math.exp(val)\n","    softmax /= sum\n","    return softmax\n","\n","def self_attention(_Q, _K, _V, _d, _bidir=False):\n","    # make masked QK^T first\n","    # upper triangle entries are set to -infinity\n","    QK = np.dot(_Q, _K)\n","    if not _bidir:\n","        for row in range(np.shape(QK)[0]):\n","            for col in reversed(range(row, np.shape(QK)[1])):\n","                QK[row][col] = float('-inf')\n","    QK /= math.sqrt(_d)\n","    softmax = compute_softmax(QK)\n","    # Y = np.dot(softmax, _V)\n","    return np.dot(softmax, _V)\n","\n","def layer_norm(_X, _alpha=1.0, _beta=0.0):\n","    \"\"\"\n","\n","    :param _X:      input matrix, each row represent vec(x)_i\n","    :param _alpha:  learned gain value\n","    :param _beta:   learned offset value\n","    :return: new _X with normalized entries\n","    \"\"\"\n","\n","    mean = np.mean(_X, axis=1)\n","    std = np.std(_X, axis=1)\n","    return _alpha * ((_X - mean) / std) + _beta\n","\n","def activation(_input):\n","    return math.tanh(_input)\n","\n","# X are tokens of the input sequence into a single matrix\n","# each row of X is the embedding of ONE token of the input\n","X = np.zeros((3,2))\n","W_Q = np.zeros((3,2))\n","W_K = np.zeros((3,2))\n","W_V = np.zeros((3,2))\n","# Query\n","Q = np.dot(np.transpose(X), W_Q)\n","# Key\n","K = np.dot(np.transpose(X), W_K)\n","# Value\n","V = np.dot(np.transpose(X), W_V)\n"]}]}