{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlaLsneiTtLt+inpSIC3BM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"DgefJi3XRzox","executionInfo":{"status":"ok","timestamp":1713075895189,"user_tz":-330,"elapsed":2324,"user":{"displayName":"Aishwarya S Yeli","userId":"06362993246759157271"}}},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","import numpy as np\n","import re\n","import os\n","\n","def compute_pp(_sentence, _unigram, _bigram, _word_types):\n","\n","    N = len(_sentence)\n","    inside_root = 1.0\n","    for i in range(N - 1):\n","\n","        count_unigram = 0\n","        for uni_item in _unigram:\n","            if uni_item['word'] == _sentence[i]:\n","                count_unigram = uni_item['count']\n","\n","        matchedw1 = False\n","        for item in _bigram:\n","            if item['w1'] == _sentence[i]:\n","                matchedw1 = True\n","                matchedw2 = False\n","\n","                for subitem in item['words']:\n","                    if subitem['w2'] == _sentence[i + 1]:\n","                        matchedw2 = True\n","                        count_bigram = subitem['count']\n","                        inside_root *= 1 / (count_bigram + 1 / count_unigram + _word_types) # Laplace smoothing\n","\n","                if matchedw2 is not True:\n","                    inside_root *= 1 / (1 / count_unigram + _word_types)\n","\n","        if matchedw1 is not True:\n","            inside_root *= 1 / (1 / count_unigram + _word_types)\n","\n","    return inside_root ** (-1 / N)\n","\n","\n","def tokenize_file(_file_path):\n","    if os.path.exists(_file_path):\n","        with open(_file_path, 'r') as f:\n","            usr_input = f.read()\n","        f.close()\n","\n","        return word_tokenize(usr_input)\n","    else:\n","        print(\"File does not exist. Please make sure that the file path you provide is valid. For testing, enter 'mid-size-corpus.txt'.\")\n","        new_file_path = input('Enter a file path: ')\n","        tokenized_usr_input = tokenize_file(new_file_path)\n","\n","        return tokenized_usr_input\n","\n","\n","def remove_punctuation_tokens(_tokens):\n","\n","    for i in range(len(_tokens)):\n","        if re.match(r\"^(``|''|,|\\.|â€”|-|;|:|'|\\\")$\", _tokens[i]):\n","            _tokens[i] = '</s>'\n","            _tokens.insert(i+1, '<s>')\n","\n","    _tokens.insert(0, '<s>')\n","    _tokens.insert(len(_tokens) - 1, '</s>')\n","\n","    return _tokens\n","\n","\n","def extract_cnt(_item):\n","    return _item['count']\n","\n","def extract_highest_w2_count(_item):\n","    return _item['words'][0]['count']\n","\n","def generate_unigram(_tokens):\n","    unigram = []\n","    _tokens = remove_punctuation_tokens(_tokens)\n","    for token in _tokens:\n","        found = False\n","        for items in unigram:\n","            if token == items['word']:\n","                items['count'] += 1\n","                found = True\n","                break\n","        if not found:\n","            unigram.append({'word':token, 'count':1})\n","\n","    unigram.sort(key=extract_cnt, reverse=True)\n","\n","    print('UNIGRAM')\n","    for item in unigram:\n","        print(f\"{item['word']}:     {item['count']}\")\n","\n","    return unigram\n","\n","def generate_bigram(_tokens):\n","\n","    bigram = []\n","    _tokens = remove_punctuation_tokens(_tokens)\n","    for i in range(len(_tokens) - 1):\n","        matchedw1 = False\n","        for item in bigram:\n","            if _tokens[i] == item['w1']:\n","                matchedw1 = True\n","                matchedw2 = False\n","                for subitem in item['words']:\n","                    if _tokens[i+1] == subitem['w2']:\n","                        matchedw2 = True\n","                        subitem['count'] += 1\n","                if not matchedw2:\n","                    item['words'].append({'w2': _tokens[i+1], 'count': 1})\n","        if not matchedw1:\n","            bigram.append({'w1': _tokens[i], 'words': [{'w2': _tokens[i+1], 'count': 1}]})\n","\n","    for item in bigram:\n","        item['words'].sort(key=extract_cnt, reverse=True)\n","\n","    bigram.sort(key=extract_highest_w2_count, reverse=True)\n","    print('BIGRAM')\n","    for item in bigram:\n","        for subitem in item['words']:\n","            print(f\"{item['w1']} {subitem['w2']}:          {subitem['count']}\")\n","\n","    return bigram\n","\n","\n","def generate_sentence(_bigram):\n","\n","    sentence = ['<s>']\n","    while sentence[-1] != '</s>':\n","        probabiliy = []\n","        words = []\n","        for item in _bigram:\n","            if item['w1'] == sentence[-1]:\n","                probabiliy = np.array([item['words'][i]['count'] for i in range(len(item['words']))], dtype='float64')\n","                words = np.array([item['words'][i]['w2'] for i in range(len(item['words']))])\n","        probabiliy /= np.sum(probabiliy)\n","        next_word = np.random.choice(words, 1, p=probabiliy)[0]\n","        sentence.append(next_word)\n","\n","    string = ''\n","    for word in sentence:\n","        string += (word + ' ')\n","    print(string)\n","\n","    return sentence\n","\n","\n","# export this\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"_Qpzh4XWR0-6"},"execution_count":null,"outputs":[]}]}